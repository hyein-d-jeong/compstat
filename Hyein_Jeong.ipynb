{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "Final Project for Computational Statistics | Summer 2022, M.Sc. Economics, University of Bonn | [Hyein Jeong](https://github.com/huiren-j)\n",
    "# Comparison of Random Forest and Logistic Regression in Classification <a class= \"tocSkip\">\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook is based on data set up of the following paper:\n",
    ">Bogan, V. L., & Fernandez, J. M. (2017). How children with mental disabilities affect household investment decisions. American Economic Review, 107(5), 536-40.\n",
    "\n",
    "**Note** : To improve readibility, this notebook does not contain codes for plotting outcomes. The entire codes including plot is retained at ./src/codes.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract** This study examines the performance of parametric and non-parametric methods. Logistic regression is used as an example of parametric methods and random forest is selected as a non-parametric model for classification. The performance of each model is evaluated by three measurement: False Positive Rate, True Positive Rate and Area Under the ROC Curve. The simulation data is manipulated to produce 8 different test sample. Across samples, logistic regression outperformed random forest. As increase the number of sample size, the performance gap measured by FPR decreased. AUC shows dominant performance of logistic regression more obviously. The largest AUC values of random forest was smaller than least AUC value of logistic model. It might suggest that the well-known disadvantage of random forest which is not suitable for data satisfying parametric test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class= \"tocSkip \"></span></h1>  \n",
    "<ul class= \"toc-item \"><li><span><a href= \"#1.-Introduction \" data-toc-modified-id= \"1.-Introduction-1 \">1. Introduction</a></span></li> <li><span><a href= \"#2.-Theory Overview \" data-toc-modified-id= \"2.-Theory Overview-2 \">2. Theory Overview</a></span></li>  \n",
    "<ul class= \"toc-item \"><li><span><a href= \"#2.1. Logistic Regression \" data-toc-modified-id= \"#2.1.-Logistic Regression 2.1 \">2.1. Logistic Regression</a></span></li>  \n",
    "<li><span><a href= \"#2.2. Random Forest \" data-toc-modified-id= \"#2.2.-Random Forest 2.1 \">2.2. Random Forest</a></span></li>\n",
    "</ul><li><span><a href= \"#3.-Data \" data-toc-modified-id= \"3.-Data-3 \">3. Data</a></span></li>\n",
    "<li><span><a href= \"#4.-Simulation \" data-toc-modified-id= \"4.-Simulation-4 \">4. Simulation</a></span></li>\n",
    "<ul class= \"toc-item \"><li><span><a href= \"#4.1. Simulation: Logistic Regression \" data-toc-modified-id= \"#4.1.-Simulation: Logistic Regression 4.1 \">4.1. Simulation: Logistic Regression</a></span></li>  \n",
    "<li><span><a href= \"#4.2. Simulation: Random Forest\" data-toc-modified-id= \"#4.2.-Simulation: Random Forest 4.2 \">4.2. Simulation: Random Forest</a></span></li>\n",
    "</ul><li><span><a href= \"#5.-Analysis \" data-toc-modified-id= \"5.-Analysis-5 \">5. Analysis</a></span></li>\n",
    "<ul class= \"toc-item \"><li><span><a href= \"#5.1. Measurement \" data-toc-modified-id= \"#5.1.-Measurement 5.1 \">5.1. Measurement</a></span></li>  \n",
    "<li><span><a href= \"#5.2. False Positive Rate and True Postivie Rate \" data-toc-modified-id= \"#5.2.-False Positive Rate and True Positive Rate 5.2 \">5.2. False Positive Rate and True Postive Rate</a></span></li>\n",
    "<li><span><a href= \"#5.3. ROC curve and AUC \" data-toc-modified-id= \"#5.3.-ROC curve and AUC 5.3 \">5.3. ROC curve and AUC</a></span></li>\n",
    "</ul><li><span><a href= \"#6. Conclusion \" data-toc-modified-id= \"#6.-Conclusion 6. \">6.Conclusion</a></span></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "library(lme4)\n",
    "library(grf)\n",
    "library(stats)\n",
    "library(MASS)\n",
    "library(sandwich)\n",
    "library(lmtest)\n",
    "library(dplyr)\n",
    "library(haven)\n",
    "library(randomForest)\n",
    "library(ggplot2)\n",
    "library(ROCR)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In economic research, regression is the most commonly used method to understand the relationship between independent variables and dependent variable. Regression enables to see counterfactual situations and helps to calculate the substantial impact of a variables of interest on a social phenomenon. However, its credibility is depending on whether the assumption on the distribution of independent variables are satisfied. This characteristics underlies across parametric methods. To complement this limitation, non parametric method is postulated which does not assume distributions of each variables. Nevertheless, it becomes less powerful when the parametric test is valid, and it makes difficult to say which method is powerful than the others. Therefore, this paper will examine the performance of parametric and non parametric method with several samples.\n",
    "\n",
    "The simulation data set comes from Bogan and Fernandez (2017). The original paper studied the impact of children with mental disability on household's investment decision. The investment decision was represented by a binary variable of 1 if the household has a certain type of asset. As a main analysis method, ***Logistic Regression*** was used. Of non parametric methods, this paper selected ***Random Forest*** to compare classification performance with logistic model. Random forest is a tree based model. A regression tree selects the most efficient split point in order and as repeats the splits, it reaches terminal node where there is no more split point. Random forest is a bundle of such regression trees. Based on bootstrap sample which allows sampling of replacement, the outcome of random forest is calculated by average of outcome of trees.\n",
    "\n",
    "The evaluation was conducted based on simulation data set up according to the data of the original paper(*Bogan and Fernandez, 2017*) and the simulation data is used to produce 8 samples. The first group is baseline sample which is 70% of observations of simulation data. The second group is samples with different number of explanatory variables by their importance. The importance of variable is determined by mean decrease in Gini coefficient indicating how each variable contributes to the homogeneity of nodes and leaves in the random forest. The third group includes three samples with different sample size. Lastly, in the fourth group, there are two samples with new dummy variables of risk preference and of these two sample, the last sample went through removal of explanatory variables by their importance.\n",
    "\n",
    "The measurement tool of performance referred to a paper which compared logistic regression and random forest(Kirasich and et al.,2018). There are three values were used to evaluate performance of each method. The first one is False Positive Rate(FPR) which is the portion of being *positive* in spite of being actually *negative*. Secondly, True Positive Rate(TPR) which is known as sensitivity was used for the analysis. It shows the portion of being *positive* of actual *positive* observation. With y-axis of TPR and x-axis of FPR, a graph can be induced, called ROC curve. The last measurement tool is Area Under the Curve(AUC) which is the area underneath ROC curve. The higher AUC value means higher accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Theory Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In economic studies, linear regression is the mostly widely used tool to understand the relationship between variables of interest. However, when it comes to categorical dependent variables, it has limitation to say that the resultant line represents all data points well. Therefore, by introducing a logit transformation, the power of prediction can be improved and the regression with logit transformation is called \\textit{Logistic Regression}. In this paper, we handle with binary dependent variable.\n",
    "\n",
    "logistic regression estimate the probability of an event occurring which is the probability of Y being 1. When the probability of 1 is p(X), it can be defined as equation \\ref{eq1} and the resultant odds will be equation \\ref{eq2} where e is the base of natural logarithm and $\\beta$ are the parameters of the model.\n",
    "\\begin{align}\n",
    "    & p(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X}}{1+e^{\\beta_{0}+\\beta_{1}X}}\\label{eq1}\\tag{1}\\\\\n",
    "    & odds = \\frac{p}{1-p}  = e^{\\beta_{0}+\\beta_{1}X}\\label{eq2}\\tag{2}\n",
    "\\end{align}\n",
    "Lastly, the logistic regression equation is as following equation \\ref{eq3} and it returns s-curve to predict the dependent variable based on independent variables.\n",
    "\\begin{align}\\label{eq3}\\tag{3}\n",
    "    \\ln{\\left(\\frac{p}{1-p}\\right) = \\beta_{0}+\\beta_{1}X}\n",
    "\\end{align}\n",
    "\n",
    "Since this method is fit in categorical variable, it is a useful for classification. The paper(Bogan and Fernandez, 2017) where this study collected data conducted their analysis on the relationship between household investment decision and children with mental disability with logistic regression. The major limitation of logistic regression is the assumption of linearity between the dependent variable and the independent variables. This assumption is underlying across parametric methods. Therefore, this study apply *random forest*,a non parametric method,to complement the limitation of logistic regression in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is a machine learning methodology widely used across discipline. Random forest is a non-parametric and tree based method. Based on a bootstrap sample, it trains each tress and when the number of trees grows, predictions are resulted from average over trees. As an ensemble of regression trees, it complements over fitting issue in using single tree.\n",
    "\n",
    "<img src=\"figure/decision_tree_df1.png\" width=\"400\" height=\"100\" ><br>\n",
    "**<p style=\"text-align: center;\">Figure 1: An Example of Regression Tree</p>**\n",
    "\n",
    "Figure 1 shows an example of a regression tree with the original data set. It is generated by a simple regression on a binary dependent variable which indicates if a household has a safe asset or not.\n",
    "\\begin{align}\n",
    "    WTRBONDS_{i} = AGEOFHEAD_{i} + lninc_{i} + EDUCHD_{i}\n",
    "\\end{align}\n",
    ",where *AGEOFHEAD* is the age of a household, *lninc* is a household's income and *EDUCHD* is the number of years of education of a household. This paper used \\texttt{randomForest} package in \\texttt{R}. the default number of tree $N$ is 500 and *mtry* which indicates variables randomly samples as candidates at each split is set to $\\lceil{p}/{3}\\rceil$ where $p$ is the maximum number of possible direction for splitting.\n",
    "\n",
    "To state random forest mathematically(Biau and Scornet, 2016). A random forest guided tour. Test, 25(2), 197-227.}, random forest comprised of N randomized regression trees and the aim of this algorithm is to predict $Y \\in \\mathbb{R}$ with observation $\\mathbf{X}\\in \\chi \\subset \\mathbb{R}$ based on a regression function $f(\\mathbf{x})=\\mathbb{E}[Y|\\mathbf{X}=\\mathbf{x}]$. For each training sample has prediction functions $f_{n}: \\chi \\rightarrow \\mathbb{R}$ where n is the size of train set, i.e., a train set $S_{n} = ((\\mathbf{X}_{1},Y_{1}), ... ,(\\mathbf{X}_{n},Y_{n}))$. Of this ensemble of regression trees, $i-th$ tree has a predicted value $f_{n}(\\mathbf{x};\\Theta_{i}, S_{n})$ where $\\Theta_{1}, ... ,\\Theta_{N}$ are independent random variables.\n",
    "\n",
    "In binary classification, function $f_{n}$is called as a classifier which predicts $Y \\in \\{0,1\\}$ by following majority estimate of trees using a measurable function of $\\mathbf{X}$ and $S_{n}$. In detail,\n",
    "\n",
    "$$\\begin{equation}\n",
    "f_{N,n}(\\mathbf{x};\\Theta_{1},...,\\Theta_{N},S_{n}) =  \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "    1 & \\mbox{if} \\frac{1}{M} \\sum_{i=1}^{N}f_n(\\mathbf{x};\\Theta_{i}, S_{n}) > 1/2\\\\ \n",
    "    0 & \\mbox{otherwise}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\end{equation}$$\n",
    "\n",
    "The original paper (Bogan and Fernandez, 2017) used logistic regression to see the effects of children with mental disability on the households' investment decision which is indicated by a binary variable. In many previous studies, non parametric methods proved an improved predictive power compared to parametric methods(Hong et al., 2020). Therefore, the application of random forest for the data analyzed based on logistic regression is expected to show better prediction on household's investment decision depending on the children's mentality status. However, some says that there is a trade-off in choosing one between those methods. For example, while random forest might return high variability in prediction and low bias, logistic regression can result in higher bias with lower variability(Kirasich et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This study benchmarked data of \\textit{Bogan and Fernandez, 2017} which investigated the impact of children with mental disability on household's investment decision. It originally from the biennial Panel Survey of Income Dynamics(PSID). It contains socioeconomic status and family structure and the authors connected it with another data, called Child Development Supplement(CDS) which includes information about children having mental- or physical disability. The data covers from 1999 to 2011 with 11763 observations.\n",
    "\n",
    "The main analysis of the original paper was conducted with a binary dependent variable, dummy variable of children with mental disability, 27 control variables and year fixed effects. The baseline model in original paper was \n",
    "\\begin{align}\n",
    "    OWN_{it} = \\beta_{0} + \\beta_{1}SpecialNeeds_{it} + \\beta_{k}\\overline{X}_{it} + \\beta_{h}Z_{it} + \\eta_{t} + \\epsilon_{it}\n",
    "\\end{align}\n",
    "where *OWN* is 1 if the household has a certain type of asset; *SpecialNeeds* is 1 if there is a children with mental disability; $\\overline{X}$ is a matrix containing children's characteristic; *Z* is a matrix of household control variables. Based on this original setting, there was some manipulation of structure in regression in this paper. The main changes is that analysis was conducted only for 1999 to eliminate disadvantage resulted from using panel data in random forest and to have precise comparison its performance with logistic regression.\n",
    "\n",
    "To explain **Data Generating Process**, the first step was generating 20 independent dummy variables. Each dummy variable was randomly produced based on the probability of 1 for each variable in the original data in 1999(n=1698). The number of children was measured by four dummy variables: *onechild, twochild, threechild* and *fourormorechild*. In this case, the variable called number of children was generated first and then it was divided into four dummy variables by one hot encoding. *Risk Control* variable which indicates groups by risk preference also went through same procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1698"
      ],
      "text/latex": [
       "1698"
      ],
      "text/markdown": [
       "1698"
      ],
      "text/plain": [
       "[1] 1698"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# original data \n",
    "df <- read_dta(\"data/Data_BoganFernandezAER.dta\")\n",
    "colnames(df) <-sub(\"_\", \"\", colnames(df))\n",
    "\n",
    "df1 <- df %>% filter(year == 1999)\n",
    "nrow(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nrow(df1) #1698\n",
    "set.seed(10)\n",
    "\n",
    "#1. dummylist\n",
    "\n",
    "### control and dummy\n",
    "controls <- c('specialneeds2', 'specialneeds4', 'grad', 'onechild', 'twochild', 'threechild', 'fourormorechild', 'conditions', 'healthins', 'married', 'divorce', 'young', 'AGEOFHEAD', 'EDUCHD', 'black', 'female', 'own1', 'unemphd', 'manager', 'finance', 'lninc', 'IMPIRA2', 'benefitpension', 'contributionpension', 'WTRINHERITANCE', 'regionMW', 'regionW', 'regionNE')\n",
    "\n",
    "### select all dummies\n",
    "dummy.list <- c(names(df1%>%select_if(~ all(. %in% 0:1))))\n",
    "ctrl.dummy <- dummy.list[dummy.list %in% controls]\n",
    "\n",
    "### remove variables of the number of children\n",
    "num.child <- c('onechild','twochild','threechild','fourormorechild')\n",
    "\n",
    "ctrl.dummy <- ctrl.dummy[! ctrl.dummy %in% num.child]\n",
    "\n",
    "# 2. simulation df with dummies\n",
    "## generate df for controls\n",
    "df.sim.dum <- data.frame(n)\n",
    "sim.dum <- c()\n",
    "for (val in ctrl.dummy){\n",
    "    nam <- paste(\"prob\",val, sep = \"\")\n",
    "    assign(nam, paste0(prop.table(table(df[val])))[[2]])\n",
    "    nam <- as.double(get(nam),digits = 5)\n",
    "    \n",
    "    nam2 <- paste0(val, \"_sim\")\n",
    "    assign(nam2, rbinom(n,1,nam))\n",
    "    \n",
    "    dum.col<- data.frame(nam2 = c(get(nam2))) # convert it to dataframe\n",
    "    df.sim.dum <- cbind(df.sim.dum, dum.col) # append a new col of dummy to existing df\n",
    "    \n",
    "    sim.dum <- append(sim.dum, nam2) #save variable name for col names\n",
    "}\n",
    "\n",
    "df.sim.dum[1] <- NULL #drop the first column (=n)\n",
    "colnames(df.sim.dum) <- sim.dum # set col names\n",
    "\n",
    "# the number of child\n",
    "\n",
    "#count probability of each number of children in original data (= (sum of col)/(nrow(df1)) )\n",
    "#make categorical variable according to the probability\n",
    "#use model.matrix of the categorial var\n",
    "\n",
    "prob1<- prop.table(table(df1['onechild']))[[2]]\n",
    "prob2<- prop.table(table(df1['twochild']))[[2]]\n",
    "prob3<- prop.table(table(df1['threechild']))[[2]]\n",
    "prob4<- prop.table(table(df1['fourormorechild']))[[2]]\n",
    "\n",
    "df.sim.dum$numchild_sim <- sample(c(0:3),size = n,prob = c(prob1, prob2, prob3, prob4),replace = TRUE)\n",
    "\n",
    "numchild_exp = model.matrix(~ factor(df.sim.dum$numchild_sim) + 0) \n",
    "colnames(numchild_exp)[1]<-\"onechild_sim\"\n",
    "colnames(numchild_exp)[2]<-\"twochild_sim\"\n",
    "colnames(numchild_exp)[3]<-\"threechild_sim\"\n",
    "colnames(numchild_exp)[4]<-\"fourormorechild_sim\"\n",
    "\n",
    "df.sim.dum$numchild_sim<-NULL\n",
    "df.sim.dum <- cbind(df.sim.dum,numchild_exp)\n",
    "\n",
    "\n",
    "### apply as.factor for all dummy columns \n",
    "col <- names(df.sim.dum)\n",
    "for (c in col){\n",
    "    if (is.factor(df.sim.dum[[c]]) == FALSE){\n",
    "        df.sim.dum[[c]] <- as.factor(df.sim.dum[[c]]) #for loop: use bracket [[]] to call columns\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, categorical variables was made with the probability for each levels in the original data. In addition, for some discrete variable and non categorical variables like *Age of Household* or *Educational years*, their values were extracted by setting same range with the one in original data. Thirdly, the only continuous variable *lninc*, a proxy of income, was resulted from a normal distribution whose mean and standard variance follows the one in original data.\n",
    "\n",
    "This paper mainly examines performance of random forest and logistic regression on classification. For evaluation, eight types of sample was used and it can divided into four groups as described in Table 1. The first sample is 70\\% of the simulation data set which will be called as *Baseline sample*. The second and third sample is about removal of explanatory variables whose mean decrease in Gini coefficient is the least one. **The mean decrease in Gini coefficient** means how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The fourth to sixth sample is about the manipulation in sample size. Compared to baseline sample, the fourth sample *Small Sample* is less than baseline sample and the other two samples are larger than baseline sample. In Group 4, there are two samples produced by addition of explanatory variables to the baseline sample. The added explanatory variables are 6 dummy variables *Risk Control* of risk preference which is generated through one-hot encoding of a categorical variable. The second sample in Group 4 dropped 5 least mean decrease in Gini coefficient variables that re-calculated including risk control variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nondum <- controls[! controls %in% ctrl.dummy & ! controls %in% num.child] \n",
    "\n",
    "# Categorial or integer vars\n",
    "func.nondum <- function(df, varname, factors = FALSE){\n",
    "    prob.gen <- prop.table(table(df[varname]))\n",
    "    a = length(prob.gen) -1 # to match index and categorial value\n",
    "    \n",
    "    prob.list <- c()\n",
    "    for (i in 0:a){\n",
    "        nam <- paste(\"prob\", as.character(i), sep = \"\")\n",
    "        assign(nam, prob.gen[[i+1]])\n",
    "        nam <- as.double(get(nam))\n",
    "\n",
    "        prob.list<- append(prob.list, nam)\n",
    "    }\n",
    "    if (factors == FALSE){\n",
    "    sim <- sample( dplyr::count(df1,get(paste(varname)))[[1]],size = n, prob = prob.list, replace = TRUE)\n",
    "    } else {\n",
    "    sim <- as.factor(sample(c(min(df[varname]):max(df[varname])),size = n, prob = prob.list, replace = TRUE))\n",
    "    }\n",
    "    return(sim)\n",
    "}\n",
    "\n",
    "conditions_sim <- func.nondum(df1, 'conditions', factors = TRUE)\n",
    "AGEOFHEAD_sim <- func.nondum(df1, 'AGEOFHEAD')\n",
    "EDUCHD_sim <- func.nondum(df1,'EDUCHD')\n",
    "\n",
    "df.sim <- cbind(df.sim.dum, conditions_sim, AGEOFHEAD_sim, EDUCHD_sim)\n",
    "\n",
    "# continuous vars\n",
    "val = 'lninc'\n",
    "\n",
    "m <- mean(df[[val]],na.rm = TRUE)\n",
    "std <- sd(df[[val]],na.rm = TRUE)\n",
    "nam <- paste0(val,\"_sim\")\n",
    "assign(nam, c(round(abs(rnorm(n,m,std)), 5)))\n",
    "\n",
    "df.sim <- cbind(df.sim, lninc_sim)\n",
    "\n",
    "# Y : WTRBONDS\n",
    "df.sim$Y <- rbinom(n,1,prop.table(table(df1['WTRBONDS']))[[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1. Type of Sample Sets**\n",
    "\n",
    "| Group | Sample               | Description                                                      |\n",
    "|-------|:---------------------|:-----------------------------------------------------------------|\n",
    "|Group1 | Baseline Sample      | Baseline data with 70\\% observations of simulation data (n=1188) |\n",
    "|Group2 | Explanatory Sample 1 | Dropped 5 explanatory variables based on the mean decrease in Gini coefficient value|\n",
    "|       | Explanatory Sample 2 | Dropped 9 explanatory variables based on the mean decrease in Gini coefficient value|\n",
    "|Group3 | Small Sample         | Reduced the size of baseline sample (n=832) |\n",
    "|       | Medium Sample        | Increased the size of baseline sample (n=1494) |\n",
    "|       | Large Sample         | Increased the size of baseline sample (n=1647) |\n",
    "|Group 4| Risk Sample 1        | Added *Risk Control* variables                 |\n",
    "|       | Risk Sample 2        | Dropped 5 explanatory variables from Risk Sample based on the mean decrease in Gini coefficient value|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group1. Baseline Trainset\n",
    "set.seed(100)\n",
    "train <- sample(nrow(df.sim), 0.7*nrow(df.sim), replace = FALSE) #sample(range, sample size) replace = TRUE if sample size > range\n",
    "trainset <- df.sim[train,]\n",
    "validset <- df.sim[-train,]\n",
    "\n",
    "# Baseline Model\n",
    "model1<- randomForest(factor(Y)~., data = trainset, importance = TRUE, proximity = TRUE)\n",
    "\n",
    "# Group2. Explanatory Sample1\n",
    "impt <- as.data.frame(importance(model1))\n",
    "impt <- impt[order(impt$MeanDecreaseGini),]\n",
    "remove <- c(row.names(impt[1:5,]))\n",
    "trainset2 <- trainset[!colnames(trainset) %in% remove]\n",
    "\n",
    "# Group2. Explanatory Sample2\n",
    "remove2 <- c(row.names(impt[1:9,]))\n",
    "trainset3 <- trainset[!colnames(trainset) %in% remove2]\n",
    "\n",
    "# Gropu3. Small, Medium, Large Sample\n",
    "train.s <- sample(nrow(trainset), 0.3*nrow(trainset), replace = FALSE) #reduce trainset size upto 70%\n",
    "train.m <- sample(nrow(validset), 0.6*nrow(validset), replace = FALSE) #sample(range, sample size) replace = TRUE if sample size > range\n",
    "train.l <- sample(nrow(validset), 0.9*nrow(validset), replace = FALSE)\n",
    "\n",
    "trainset.s <- trainset[-train.s,]\n",
    "trainset.m <- rbind(validset[train.m,],trainset)\n",
    "trainset.l <- rbind(validset[train.l,],trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group4. Add Risk Control Variable\n",
    "prob1<- prop.table(table(df1['riskgrp1']))[[2]]\n",
    "prob2<- prop.table(table(df1['riskgrp2']))[[2]]\n",
    "prob3<- prop.table(table(df1['riskgrp3']))[[2]]\n",
    "prob4<- prop.table(table(df1['riskgrp4']))[[2]]\n",
    "prob5<- prop.table(table(df1['riskgrp5']))[[2]]\n",
    "prob6<- prop.table(table(df1['riskgrp6']))[[2]]\n",
    "\n",
    "df.sim2 <- df.sim\n",
    "df.sim2$riskgrp_sim <- sample(c(0:5),size = n,prob = c(prob1, prob2, prob3, prob4, prob5, prob6),replace = TRUE)\n",
    "\n",
    "riskgrp_exp = model.matrix(~ factor(df.sim2$riskgrp) + 0) \n",
    "colnames(riskgrp_exp)[1]<-\"riskgrp1_sim\"\n",
    "colnames(riskgrp_exp)[2]<-\"riskgrp2_sim\"\n",
    "colnames(riskgrp_exp)[3]<-\"riskgrp3_sim\"\n",
    "colnames(riskgrp_exp)[4]<-\"riskgrp4_sim\"\n",
    "colnames(riskgrp_exp)[5]<-\"riskgrp5_sim\"\n",
    "colnames(riskgrp_exp)[6]<-\"riskgrp6_sim\"\n",
    "\n",
    "df.sim2$riskgrp_sim<-NULL\n",
    "df.sim2<- cbind(df.sim2, riskgrp_exp)\n",
    "\n",
    "# Group4. Risk Sample1\n",
    "trainset.r1 <- df.sim2[train,]\n",
    "model.r1<- randomForest(factor(Y)~., data = trainset.r1, importance = TRUE, proximity = TRUE)\n",
    "\n",
    "# Group4. Risk Sample2\n",
    "impt.r <- as.data.frame(importance(model.r1))\n",
    "impt.r <- impt.r[order(impt.r$MeanDecreaseGini),]\n",
    "remove.r <- c(row.names(impt.r[1:5,]))\n",
    "\n",
    "trainset.r2 <- trainset.r1[!colnames(trainset) %in% remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Simulation: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1 <- glm(factor(Y) ~. , data = trainset, family = binomial(link = 'logit')) # baseline trainset\n",
    "lm2 <- glm(factor(Y) ~. , data = trainset2, family = binomial(link = 'logit')) # drop 5 x (5 least important vars)\n",
    "lm3 <- glm(factor(Y) ~. , data = trainset3, family = binomial(link = 'logit')) # drop 9 x (9 least important vars)\n",
    "lm4 <- glm(factor(Y) ~. , data = trainset.s, family = binomial(link = 'logit')) # smaller than train set\n",
    "lm5 <- glm(factor(Y) ~. , data = trainset.m, family = binomial(link = 'logit')) # a little bigger than trainset\n",
    "lm6 <- glm(factor(Y) ~. , data = trainset.l, family = binomial(link = 'logit')) # lager than trainset\n",
    "\n",
    "lm.r1 <- glm(factor(Y) ~. , data = trainset, family = binomial(link = 'logit')) # added risk group\n",
    "lm.r2 <- glm(factor(Y) ~. , data = trainset, family = binomial(link = 'logit')) # added risk group + dropped 5 x (by new importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Simulation: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = factor(Y) ~ ., data = trainset, importance = TRUE,      proximity = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 500\n",
       "No. of variables tried at each split: 5\n",
       "\n",
       "        OOB estimate of  error rate: 22.22%\n",
       "Confusion matrix:\n",
       "    0 1 class.error\n",
       "0 924 3 0.003236246\n",
       "1 261 0 1.000000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = factor(Y) ~ ., data = trainset2, importance = TRUE,      proximity = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 500\n",
       "No. of variables tried at each split: 4\n",
       "\n",
       "        OOB estimate of  error rate: 22.05%\n",
       "Confusion matrix:\n",
       "    0 1 class.error\n",
       "0 925 2 0.002157497\n",
       "1 260 1 0.996168582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = factor(Y) ~ ., data = trainset3, importance = TRUE,      proximity = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 500\n",
       "No. of variables tried at each split: 4\n",
       "\n",
       "        OOB estimate of  error rate: 22.39%\n",
       "Confusion matrix:\n",
       "    0 1 class.error\n",
       "0 921 6 0.006472492\n",
       "1 260 1 0.996168582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = factor(Y) ~ ., data = trainset.s, importance = TRUE,      proximity = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 500\n",
       "No. of variables tried at each split: 5\n",
       "\n",
       "        OOB estimate of  error rate: 21.51%\n",
       "Confusion matrix:\n",
       "    0 1 class.error\n",
       "0 653 1 0.001529052\n",
       "1 178 0 1.000000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = factor(Y) ~ ., data = trainset.m, importance = TRUE,      proximity = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 500\n",
       "No. of variables tried at each split: 5\n",
       "\n",
       "        OOB estimate of  error rate: 22.69%\n",
       "Confusion matrix:\n",
       "     0 1 class.error\n",
       "0 1155 2 0.001728608\n",
       "1  337 0 1.000000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = factor(Y) ~ ., data = trainset.l, importance = TRUE,      proximity = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 500\n",
       "No. of variables tried at each split: 5\n",
       "\n",
       "        OOB estimate of  error rate: 21.86%\n",
       "Confusion matrix:\n",
       "     0 1  class.error\n",
       "0 1287 1 0.0007763975\n",
       "1  359 0 1.0000000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = factor(Y) ~ ., data = trainset.r1, importance = TRUE,      proximity = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 500\n",
       "No. of variables tried at each split: 5\n",
       "\n",
       "        OOB estimate of  error rate: 22.05%\n",
       "Confusion matrix:\n",
       "    0 1 class.error\n",
       "0 926 1 0.001078749\n",
       "1 261 0 1.000000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = factor(Y) ~ ., data = trainset.r2, importance = TRUE,      proximity = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 500\n",
       "No. of variables tried at each split: 5\n",
       "\n",
       "        OOB estimate of  error rate: 22.31%\n",
       "Confusion matrix:\n",
       "    0 1 class.error\n",
       "0 923 4 0.004314995\n",
       "1 261 0 1.000000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Group1 Baselin Model\n",
    "model1<- randomForest(factor(Y)~., data = trainset, importance = TRUE, proximity = TRUE)\n",
    "model1\n",
    "\n",
    "#Group2 Explanatory Variable Model\n",
    "model2<- randomForest(factor(Y)~., data = trainset2, importance = TRUE, proximity = TRUE)\n",
    "model2\n",
    "\n",
    "model3<- randomForest(factor(Y)~., data = trainset3, importance = TRUE, proximity = TRUE)\n",
    "model3\n",
    "\n",
    "#Group3 Sample Size Model\n",
    "model4<- randomForest(factor(Y)~., data = trainset.s, importance = TRUE, proximity = TRUE)\n",
    "model4\n",
    "\n",
    "model5<- randomForest(factor(Y)~., data = trainset.m, importance = TRUE, proximity = TRUE)\n",
    "model5\n",
    "\n",
    "model6<- randomForest(factor(Y)~., data = trainset.l, importance = TRUE, proximity = TRUE)\n",
    "model6\n",
    "\n",
    "#Group4 Risk Control Model\n",
    "model.r1<- randomForest(factor(Y)~., data = trainset.r1, importance = TRUE, proximity = TRUE)\n",
    "model.r1\n",
    "\n",
    "model.r2<- randomForest(factor(Y)~., data = trainset.r2, importance = TRUE, proximity = TRUE)\n",
    "model.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAABNTU1h0E9oaGh8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\n",
      "text/plain": [
       "Plot with title \"model.r2\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(mfrow = c(2,4))\n",
    "plot(model1)\n",
    "plot(model2)\n",
    "plot(model3)\n",
    "plot(model4)\n",
    "plot(model5)\n",
    "plot(model6)\n",
    "plot(model.r1)\n",
    "plot(model.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analysis\n",
    "## 5.1. Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of random forest classification, this paper selected three criteria: false positive rate, true positive rate and area under the curve. **False Positive Rate**(FPR) is defined as the portion of being incorrectly assigned as positive but actually negative. On the other hand, **True Positive Rate**(TPR) which is known as sensitivity, indicates the portion of being detected as positive when it is actually positive. The curve with y-axis of TPR and x-axis of FPR is called **ROC curve** and the two-dimensional area underneath the entire ROC curve is defined as **Area Under the ROC curve**(AUC). The higher AUC means higher performance of a model because it means TPR of the model which means the ratio of conducting correct classification is more frequent than incorrect classification(FPR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 2. Performance Measurement**\n",
    "\n",
    "| Measurement | Description |\n",
    "|-------------|:-------------:|\n",
    "|False Positive Rate | $\\frac{FP}{FP+TN}$|\n",
    "|True Positive Rate | $\\frac{TP}{TP+FN}$|\n",
    "|ROC Curve | y: TPR, x: FPR|\n",
    "|Area Under the ROC curve | the scale of area under the ROC curve|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. False Positive Rate and True Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Figure 2 shows FPR and TPR value of random forest and logistic regression with baseline sample respectively. It manifests that the logistic regression performs better than random forest when it comes to those values. In detail, logistic regression has smaller case of incorrect classification and shows remarkably better performance in correct classification than random forest.\n",
    "\n",
    "**Figure 2. Group1: Basline Model**\n",
    "\n",
    "| Baseline FPR | Baseline TPR|\n",
    ":----:|:---:\n",
    "<img src=\"figure/baseline_comparison_fpr.png\" width=\"300\" height=\"100\" ><br>|<img src=\"figure/baseline_comparison_tpr.png\" width=\"300\" height=\"100\" ><br>\n",
    "\n",
    "The second group is the sample with adjustment in explanatory variables. According to the level of mean decrease in Gini Coefficient, the first sample in this group removed 5 explanatory varibles:*WTRINHERITANCE, fourormorechild, healthins, finance* and *manager*. *WTRINTERITANCE* is a dummy variable of 1 if the household has received an inheritance; *fourormorechild* is also a dummy variable of 1 if the household has four or more children; *healthins* indicates if the household has health insurance; *finance* is 1 if the household is employed in the financial sector; *manager* is 1 if the household is employed in a managerial position and zero otherwise. In the second sample in this group, four more explanatory variables are removed, which are, *regionN, unemphd, onechild* and *threechild*. *RegionN* is a proxy for the household locating in North region and *unemphd* is 1 if the household is unemployed. The result in Figure 3 shows that logistic regression showed better performance across the both samples. However, as the explanatory variables are removed, the difference in performance measurement decreased, in particularly, FPR value in the second sample returns similar value.\n",
    "\n",
    "**Figure 3. Group 2: Changes in Explanatory Variables**\n",
    "\n",
    "| Group 2 FPR | Group 2 TPR|\n",
    ":----:|:---:\n",
    "<img src=\"figure/explanatory_comparison_fpr.png\" width=\"300\" height=\"100\" ><br>|<img src=\"figure/explanatory_comparison_tpr.png\" width=\"300\" height=\"100\" ><br>\n",
    "\n",
    "The third group consists of the samples with different sample size. FPR values were remained between 0.4 to 0.6 across samples and models and logistic model shows lower FPR over the samples which means it reached superior performance. Regaring to TPR, while resultant mean value of random forest model remained between 0.3 to 0.5, logistic model caused outcome between 0.5 to 0.6 which dominated random forest model. Therefore, in this group also showed that logistic model performed better but as the sample size increases, the performance gap was alleviated.\n",
    "\n",
    "**Figure 4. Group 3: Changes in Sample Size**\n",
    "\n",
    "| Group 3 FPR | Group 3 TPR|\n",
    ":----:|:---:\n",
    "<img src=\"figure/samplesize_comparison_fpr.png\" width=\"300\" height=\"100\" ><br>|<img src=\"figure/samplesize_comparison_tpr.png\" width=\"300\" height=\"100\" ><br>\n",
    "\n",
    "In the fourth group, new control variables were added which are dummy variable of risk preference level and the second sample of this group went through removal of 5 explanatory variables of the least mean decrease in Gini coefficient. To compare the second sample of this group with the previous sample with removal of 5 explanatory variables, the one with new control variables returns greater gap between FPR of two models. As like in the previous group, this group also manifested superior performance of logistic model over random forest.\n",
    "\n",
    "**Figure 5. Group 4: Addition of Risk Control Variables**\n",
    "\n",
    "| Group 4 FPR | Group 4 TPR|\n",
    ":----:|:---:\n",
    "<img src=\"figure/riskgrp_comparison_fpr.png\" width=\"300\" height=\"100\" ><br>|<img src=\"figure/riskgrp_comparison_tpr.png\" width=\"300\" height=\"100\" ><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. ROC Curve and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 6 display the comparison of performance of two models in more readable way. There is a 45 radius line across the grid which is $TPR(y) = FPR(x)$ The orange line represents random forest model and its ROC curve mostly underneath the diagonal line. This means the random forest returns more cases of false classification than correct classification. \n",
    "\n",
    "**Figure 6. ROC Curves**\n",
    "\n",
    "| | | | |\n",
    ":-:|:-:|:-:|:-:\n",
    "Baseline | Group 2 | Group 2 | Group 3\n",
    "<img src=\"figure/ROC1.baseline.png\" width=\"300\" height=\"100\" ><br>|<img src=\"figure/ROC2.explanatory1.png\" width=\"300\" height=\"100\" ><br>|<img src=\"figure/ROC2.explanatory2.png\" width=\"300\" height=\"100\" ><br>|<img src=\"figure/ROC3.samplesize1.png\" width=\"300\" height=\"100\" ><br>  \n",
    "Group 3 | Group 3 | Group 4| Group4  \n",
    "<img src=\"figure/ROC3.samplesize2.png\" width=\"300\" height=\"100\" ><br>|<img src=\"figure/ROC3.samplesize3.png\" width=\"300\" height=\"100\" ><br>|<img src=\"figure/ROC4.risk1.png\" width=\"300\" height=\"100\" ><br>|<img src=\"figure/ROC4.risk2.png\" width=\"300\" height=\"100\" ><br>\n",
    "\n",
    "\n",
    "To see detailed comparison, AUC values are suggested in Table 3. The higher AUC value means greater area under the ROC curve which means there are more case included in $TPR(y) > FPR(x)$. Therefore, the higher AUC value indicates better performance of the model. The sample with the highest AUC is **Small Sample** of Group 2. It returns approximately 0.62 of AUC. On the other hand, the lowest AUC was found in **Explanatory Sample 1** whose AUC is approximately 0.452. Group 2 suggest that how the performance changes according to the sample size. In random forest model, the value changes 0.453, 0.456(baseline), 0.482, 0.453. Although it shows drop in AUC in the largest sample, in general it indicates better performance as the sample size grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 3. AUC Values**\n",
    "\n",
    "| Group | Sample | Model | AUC|  \n",
    "|-------|:--------|:-------|:----|  \n",
    "|Group1 |Baseline Sample| Random Forest | 0.4562074|  \n",
    "| | |Logistic Regression | 0.5966265|  \n",
    "|Group2 | Explanatory Sample 1 | Random Forest | 0.4521052|  \n",
    "|       | |Logistic Regression | 0.5889017|  \n",
    "|        | Explanatory Sample 2 | Random Forest | 0.5024365|  \n",
    "|        | |Logistic Regression | 0.5863805|  \n",
    "|Group3 | Small Sample | Random Forest | 0.4532764|  \n",
    "|       | |Logistic Regression | 0.6241219|  \n",
    "|       | Medium Sample | Random Forest | 0.4819982|  \n",
    "|       | |Logistic Regression | 0.5825306 |  \n",
    "|       | Large Sample | Random Forest | 0.4529289|  \n",
    "|       | |Logistic Regression | 0.5713568 |  \n",
    "|Group 4| Risk Sample 1 | Random Forest | 0.4539569|  \n",
    "|       | |Logistic Regression | 0.5966265 |  \n",
    "|      | Risk Sample 2 | Random Forest | 0.4723596|  \n",
    "|      | |Logistic Regression | 0.5966265|  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to.auc <-function(model, trainset, Y){\n",
    "    pred1=predict(model,type = \"prob\")\n",
    "\n",
    "    perf = prediction(pred1[,2], trainset$Y)\n",
    "    pred = performance(perf, \"auc\")\n",
    "    \n",
    "    df.auc <-data.frame(auc =pred@y.values)\n",
    "    colnames(df.auc) <-c('auc')\n",
    "\n",
    "    return(df.auc[[1]])\n",
    "}\n",
    "\n",
    "#auc\n",
    "lm.to.auc <-function(logit.model, trainset, Y){\n",
    "    logit.pred1=predict(logit.model, type = \"response\")\n",
    "\n",
    "    logit.perf = prediction(logit.pred1, trainset$Y)\n",
    "    logit.pred = performance(logit.perf, \"auc\")\n",
    "\n",
    "    df.auc <-data.frame(auc =logit.pred@y.values)\n",
    "    colnames(df.auc) <-c('auc')\n",
    "    df.auc\n",
    "\n",
    "    return(df.auc[[1]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 8  1</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>auc</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.5966265</td></tr>\n",
       "\t<tr><td>0.5889017</td></tr>\n",
       "\t<tr><td>0.5863805</td></tr>\n",
       "\t<tr><td>0.6149022</td></tr>\n",
       "\t<tr><td>0.5742981</td></tr>\n",
       "\t<tr><td>0.5724796</td></tr>\n",
       "\t<tr><td>0.5966265</td></tr>\n",
       "\t<tr><td>0.5966265</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 8  1\n",
       "\\begin{tabular}{l}\n",
       " auc\\\\\n",
       " <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.5966265\\\\\n",
       "\t 0.5889017\\\\\n",
       "\t 0.5863805\\\\\n",
       "\t 0.6149022\\\\\n",
       "\t 0.5742981\\\\\n",
       "\t 0.5724796\\\\\n",
       "\t 0.5966265\\\\\n",
       "\t 0.5966265\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 8  1\n",
       "\n",
       "| auc &lt;dbl&gt; |\n",
       "|---|\n",
       "| 0.5966265 |\n",
       "| 0.5889017 |\n",
       "| 0.5863805 |\n",
       "| 0.6149022 |\n",
       "| 0.5742981 |\n",
       "| 0.5724796 |\n",
       "| 0.5966265 |\n",
       "| 0.5966265 |\n",
       "\n"
      ],
      "text/plain": [
       "  auc      \n",
       "1 0.5966265\n",
       "2 0.5889017\n",
       "3 0.5863805\n",
       "4 0.6149022\n",
       "5 0.5742981\n",
       "6 0.5724796\n",
       "7 0.5966265\n",
       "8 0.5966265"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 8  1</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>auc</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.4735355</td></tr>\n",
       "\t<tr><td>0.4676355</td></tr>\n",
       "\t<tr><td>0.4968609</td></tr>\n",
       "\t<tr><td>0.4408953</td></tr>\n",
       "\t<tr><td>0.4464721</td></tr>\n",
       "\t<tr><td>0.4425022</td></tr>\n",
       "\t<tr><td>0.4892476</td></tr>\n",
       "\t<tr><td>0.4876171</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 8  1\n",
       "\\begin{tabular}{l}\n",
       " auc\\\\\n",
       " <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.4735355\\\\\n",
       "\t 0.4676355\\\\\n",
       "\t 0.4968609\\\\\n",
       "\t 0.4408953\\\\\n",
       "\t 0.4464721\\\\\n",
       "\t 0.4425022\\\\\n",
       "\t 0.4892476\\\\\n",
       "\t 0.4876171\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 8  1\n",
       "\n",
       "| auc &lt;dbl&gt; |\n",
       "|---|\n",
       "| 0.4735355 |\n",
       "| 0.4676355 |\n",
       "| 0.4968609 |\n",
       "| 0.4408953 |\n",
       "| 0.4464721 |\n",
       "| 0.4425022 |\n",
       "| 0.4892476 |\n",
       "| 0.4876171 |\n",
       "\n"
      ],
      "text/plain": [
       "  auc      \n",
       "1 0.4735355\n",
       "2 0.4676355\n",
       "3 0.4968609\n",
       "4 0.4408953\n",
       "5 0.4464721\n",
       "6 0.4425022\n",
       "7 0.4892476\n",
       "8 0.4876171"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#auc: logisitc regression\n",
    "data.frame(auc = c(lm.to.auc(lm1,trainset,Y),\n",
    "lm.to.auc(lm2,trainset2,Y),\n",
    "lm.to.auc(lm3,trainset3,Y),\n",
    "lm.to.auc(lm4,trainset.s,Y),\n",
    "lm.to.auc(lm5,trainset.m,Y),\n",
    "lm.to.auc(lm6,trainset.l,Y),\n",
    "lm.to.auc(lm.r1,trainset.r1,Y),\n",
    "lm.to.auc(lm.r2,trainset.r2,Y)))\n",
    "\n",
    "#auc: Rf\n",
    "data.frame(auc = c(model.to.auc(model1,trainset,Y),\n",
    "model.to.auc(model2,trainset2,Y),\n",
    "model.to.auc(model3,trainset3,Y),\n",
    "model.to.auc(model4,trainset.s,Y),\n",
    "model.to.auc(model5,trainset.m,Y),\n",
    "model.to.auc(model6,trainset.l,Y),\n",
    "model.to.auc(model.r1,trainset.r1,Y),\n",
    "model.to.auc(model.r2,trainset.r2,Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper has its value in comparing the classification performance of two different methods: random forest and logistic regression. The measurement tools are FPR, TPR and AUC. The main analysis was conducted over 8 different samples with changes in the number of explanatory variables, sample size and addition of new variables.Across the sample sets, performance of logistic regression dominated performance of random forest regarding to both FPR and TPR. Removal of less important explanatory variables and increase in the number of sample size reduced performance gap between two models.\n",
    "\n",
    "ROC curve has TPR as y-axis and FPR as x-axis like Figure 6 and the 45 degree line is where TPR = FPR. Therefore, if a ROC curve has more points where TPR is greater than FPR, the AUC value becomes greater. While the largest auc of random forest is 0.502 of Explanatory Sample 2 in Group 2 at Table 3, the least auc of logistic regression model is 0.571 in Large Sample in Group 3 at Table 3. It implies that logistic regression performs better than random forest regardless of sample set.\n",
    "\n",
    "This study might suggest the well-know disadvantage of non parametric model which is not suitable for data where parametric test is valid. There is possibility that random forest cannot outperform logistic regression because each variable satisfies assumed distribution. If so, it postulates that the performance gap with panel data might be greater than the one measured in this study. This study used a static sample with year$= 1999$ of panel data to control disadvantage on random forest due to dynamics. Therefore, this is open question that performance of the two models in panel data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Biau, G., & Scornet, E. (2016). A random forest guided tour. Test, 25(2), 197-227.\n",
    "\n",
    "[2]Bogan, V. L., & Fernandez, J. M. (2017). How children with mental disabilities affect household investment decisions. American Economic Review, 107(5), 536-40.\n",
    "\n",
    "[3] Kirasich, K., Smith, T., & Sadler, B. (2018). Random forest vs logistic regression: binary classification for heterogeneous datasets. SMU Data Science Review, 1(3), 9.\n",
    "\n",
    "[4] Janys, L. (2022). Computational Statistics. https://github.com/LJanys/CompStat\n",
    "\n",
    "[5] Luo, H., Pan, X., Wang, Q., Ye, S., & Qian, Y. (2019, July). Logistic regression and random forest for effective imbalanced classification. In 2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC) (Vol. 1, pp. 916-917). IEEE.\n",
    "\n",
    "[6] Shah, K., Patel, H., Sanghvi, D., & Shah, M. (2020). A comparative analysis of logistic regression, random forest and KNN models for the text classification. Augmented Human Research, 5(1), 1-16.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
